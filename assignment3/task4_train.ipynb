{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple example on how you can use a jupyter notebook to train your model :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import utils\n",
    "#from dataloaders import load_cifar10\n",
    "from trainer import Trainer, compute_loss_and_accuracy\n",
    "from task2 import create_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Dataloader for task 4a)\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch\n",
    "import typing\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std =  [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "def load_cifar10(batch_size: int, validation_fraction: float = 0.1\n",
    "                 ) -> typing.List[torch.utils.data.DataLoader]:\n",
    "    # Note that transform train will apply the same transform for\n",
    "    # validation!\n",
    "    transform_train = transforms.Compose([\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "        #transforms.RandomAffine(degrees=15, scale=(0.5,2))\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    data_train = datasets.CIFAR10('data/cifar10',\n",
    "                                  train=True,\n",
    "                                  download=True,\n",
    "                                  transform=transform_train)\n",
    "\n",
    "    data_test = datasets.CIFAR10('data/cifar10',\n",
    "                                 train=False,\n",
    "                                 download=True,\n",
    "                                 transform=transform_test)\n",
    "\n",
    "    indices = list(range(len(data_train)))\n",
    "    split_idx = int(np.floor(validation_fraction * len(data_train)))\n",
    "\n",
    "    val_indices = np.random.choice(indices, size=split_idx, replace=False)\n",
    "    train_indices = list(set(indices) - set(val_indices))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    validation_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    dataloader_train = torch.utils.data.DataLoader(data_train,\n",
    "                                                   sampler=train_sampler,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_workers=2,\n",
    "                                                   drop_last=True)\n",
    "\n",
    "    dataloader_val = torch.utils.data.DataLoader(data_train,\n",
    "                                                 sampler=validation_sampler,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 num_workers=2)\n",
    "\n",
    "    dataloader_test = torch.utils.data.DataLoader(data_test,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  num_workers=2)\n",
    "\n",
    "    return dataloader_train, dataloader_val, dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class ResnetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torchvision.models.resnet18(pretrained=True)\n",
    "        self.model.fc = nn.Linear(512, 10) # No need to apply softmax,\n",
    "                                            # as this is done in nn.CrossEntropyLoss\n",
    "\n",
    "        for param in self.model.parameters(): # Freeze all parameters\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.model.fc.parameters(): # Unfreeze the last fully-connected\n",
    "            param.requires_grad = True          # layer\n",
    "\n",
    "        for param in self.model.layer4.parameters(): # Unfreeze the last 5 convolutional\n",
    "            param.requires_grad = True               # layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "    # Set the random generator seed (parameters, shuffling etc).\n",
    "    # You can try to change this and check if you still get the same result! \n",
    "    utils.set_seed(0)\n",
    "    epochs = 10\n",
    "    batch_size = 32\n",
    "    learning_rate = 5e-4\n",
    "    early_stop_count = 4\n",
    "    dataloaders = load_cifar10(batch_size)\n",
    "    model = ResnetModel()\n",
    "    trainer = Trainer(\n",
    "        batch_size,\n",
    "        learning_rate,\n",
    "        early_stop_count,\n",
    "        epochs,\n",
    "        model,\n",
    "        dataloaders\n",
    "    )\n",
    "    trainer.train()\n",
    "    create_plots(trainer, \"task4\")\n",
    "\n",
    "    #Loading best model and plotting train, val, test accuracy\n",
    "    trainer.load_best_model()\n",
    "    print(trainer.model)\n",
    "    train_loss, train_acc = compute_loss_and_accuracy(\n",
    "        dataloaders[0], trainer.model, loss_criterion=nn.CrossEntropyLoss()\n",
    "    )\n",
    "    val_loss, val_acc = compute_loss_and_accuracy(\n",
    "        dataloaders[1], trainer.model, loss_criterion=nn.CrossEntropyLoss()\n",
    "    )\n",
    "\n",
    "    test_loss, test_accuracy = compute_loss_and_accuracy(\n",
    "        dataloaders[2], trainer.model, loss_criterion=nn.CrossEntropyLoss()\n",
    "    )\n",
    "    print('Performance for best model:')\n",
    "    print('train loss, ', train_loss)\n",
    "    print('train accuracy: ', train_acc)\n",
    "    print('validation loss: ', val_loss)\n",
    "    print('validation accuracy: ', val_acc)\n",
    "    print('test_loss: ', test_loss)\n",
    "    print('test_accuracy: ', test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}